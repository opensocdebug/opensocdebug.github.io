---
title: "Open System-on-Chip Debug (OSD) Overview"
---

<div class="osd-doc-container">
  <h1 class="osd-doc-header">{{ page.title }}</h1>
  <hr/>

  <!-- document navigation sidebar -->
    <div class="col-md-3">
    <div data-spy="affix" data-offset-top="50" id="osd-docnav" class="osd-docnav">
      <nav class="osd-docnav-nav nav hidden-print" role="complementary">
        <ul>
        <li><a href="#introduction">Introduction</a><ul>
        <li><a href="#about-open-soc-debug">About Open SoC Debug</a></li>
        <li><a href="#scope">Scope</a></li>
        <li><a href="#current-status">Current Status</a></li>
        <li><a href="#about-this-document">About This Document</a></li>
        <li><a href="#osd-contributions-and-licensing">OSD Contributions and Licensing</a><ul>
        <li><a href="#specification-license">Specification License</a></li>
        </ul></li>
        <li><a href="#revision-history">Revision History</a><ul>
        <li><a href="#version-2016.1-preview-2-to-be-released">Version 2016.1, Preview 2 (to be released)</a></li>
        <li><a href="#version-2016.1-preview-1-released-2016-02-01">Version 2016.1, Preview 1 (released 2016-02-01)</a></li>
        </ul></li>
        </ul></li>
        <li><a href="#high-level-features">High-Level Features</a><ul>
        <li><a href="#run-control-debug">Run-Control Debug</a></li>
        <li><a href="#tracing">Tracing</a></li>
        <li><a href="#memory-access">Memory Access</a></li>
        <li><a href="#system-discovery">System Discovery</a></li>
        <li><a href="#timestamping">Timestamping</a></li>
        <li><a href="#security-and-authentication">Security and Authentication</a></li>
        </ul></li>
        <li><a href="#osd-by-example">OSD By Example</a><ul>
        <li><a href="#osd-for-run-control-debugging">OSD for Run-Control Debugging</a></li>
        <li><a href="#osd-for-tracing">OSD for Tracing</a></li>
        </ul></li>
        <li><a href="#the-open-soc-debug-architecture">The Open SoC Debug Architecture</a><ul>
        <li><a href="#debug-modules">Debug Modules</a><ul>
        <li><a href="#register-access">Register Access</a></li>
        <li><a href="#debug-events">Debug Events</a></li>
        <li><a href="#trace-modules">Trace Modules</a></li>
        <li><a href="#clock-domains">Clock Domains</a></li>
        <li><a href="#overflow-handling">Overflow Handling</a></li>
        </ul></li>
        <li><a href="#transport-and-switching">Transport and Switching</a></li>
        <li><a href="#physical-interface">Physical Interface</a></li>
        <li><a href="#host-software">Host Software</a></li>
        </ul></li>
        <li><a href="#basic-debug-modules">Basic Debug Modules</a><ul>
        <li><a href="#host-interface-module-him">Host Interface Module (HIM)</a></li>
        <li><a href="#host-authentication-module-ham">Host Authentication Module (HAM)</a></li>
        <li><a href="#system-control-module-scm">System Control Module (SCM)</a></li>
        <li><a href="#core-debug-module-cdm">Core Debug Module (CDM)</a></li>
        <li><a href="#core-trace-module-ctm">Core Trace Module (CTM)</a></li>
        <li><a href="#software-trace-module-stm">Software Trace Module (STM)</a></li>
        <li><a href="#memory-access-module-mam">Memory Access Module (MAM)</a></li>
        <li><a href="#debug-processor-module-dpm">Debug Processor Module (DPM)</a></li>
        <li><a href="#device-emulation-module-dem">Device Emulation Module (DEM)</a></li>
        </ul></li>
        </ul>
      </nav>
    </div>
  </div>
  
  <!-- document content -->
  <div class="col-md-9">
    <h1 id="introduction">Introduction</h1>
    <h2 id="about-open-soc-debug">About Open SoC Debug</h2>
    <p>Systems-on-Chip (SoCs) have become embedded deeply into our lives. Most of the time we enjoy the way they serve their purpose without getting in the way. Until they don’t. In those moments, we as engineers are reminded of the complex interplay between software and hardware in SoCs. We might pose questions like “How does my software execute on the chip?” or “Why is it showing this exact behavior?” To answer these questions we need insight into the system that executes the software. We gain this insight through the debug infrastructure integrated into the SoC.</p>
    <p>Even though debug infrastructure is an essential part of any SoC design, most people consider creating it more of a necessary chore than an exciting endeavor. Therefore, most vendors today include debug infrastructure that follows one of two major specifications: <a href="http://www.arm.com/products/system-ip/debug-trace/">ARM CoreSight</a> and <a href="http://nexus5001.org/">NEXUS 5001</a> (officially called “IEEE-ISTO 5001”). Unfortunately, none of these specifications are fully open, they cannot be used without any money involved.</p>
    <p>The Open SoC Debug (OSD) specification was created to close this gap. Three key messages guide its design.</p>
    <ul>
    <li><strong>OSD is a truly open (source) specification.</strong> Without any committee membership required or royalty fees to be payed, anyone can freely
    <ul>
    <li>share and modify the specification itself, and</li>
    <li>create and distribute implementations of the specification for any purpose.</li>
    </ul></li>
    <li><strong>OSD is for debugging and tracing.</strong> A debugging infrastructure by itself is not a solution, but a toolbox providing the right tool for the task. Some bugs are best hunted using run-control debugging, some are better found using tracing. OSD supports both, enabling hardware and software developers to pick what’s best for their needs.</li>
    <li><strong>OSD provides the common and enables the special.</strong> SoCs came to live because they allow reuse of components and specialization at the same time, letting hardware designers focus on the unique challenges without re-inventing the wheel. OSD follows this lead. Common IP blocks, interfaces and software tools can be re-used, and multiple extension vectors allow for easy customization where necessary.</li>
    </ul>
    <h2 id="scope">Scope</h2>
    <p>By implementing Open SoC Debug, a SoC gains the following features (to a varying and implementation-defined degree).</p>
    <ul>
    <li>Support for run-control debugging, i.e. setting breakpoints and watchpoints and reading register values. In short, all you need to attach a debugger like GDB to the SoC.</li>
    <li>Support for tracing, i.e. non-intrusively observing the program execution on the SoC.</li>
    <li>Support for remotely controlling the SoC during development, e.g. starting the CPUs, resetting the system, and reading and writing the memories.</li>
    </ul>
    <p>To provide these features, this specification defines - an extensible debug system architecture, covering both hardware and host software, - templates with well-defined interfaces for debug and trace IP blocks (“debug modules”), - a set of common debug modules for the most frequent run-control debug and tracing tasks, - a host-side software programming interface (API) for debug tools to interact with an OSD-enabled debug system.</p>
    <p>In addition, implementations of many components described in the OSD specification are made available under a permissive open source license which can be used directly in custom designs.</p>
    <h2 id="current-status">Current Status</h2>
    <p>OSD is an evolving effort. Currently, we target the first release of the base specification and module specification, that contain the following parts:</p>
    <ul>
    <li>Basic interfaces and transport protocols</li>
    <li>A generic and mandatory memory map for all debug modules to allow enumeration, capabilities and versioning</li>
    <li>Basic modules for run-control and trace debugging</li>
    </ul>
    <p>This is just the start that covers the very basic functionality, but more features are planned to be added to the specification in the near future: tracing to memory instead of host, device traces, module triggering, cross-triggers, on-chip aggregation and filtering, sophisticated interconnects, just to mention a few.</p>
    <h2 id="about-this-document">About This Document</h2>
    <p>This document gives an overview of Open SoC Debug. The goal is to provide interested designers SoC hardware components as well as developers of debugging software tools a good understanding of the overall picture and the reasoning behind the design of OSD. This document is not the specification itself. Please refer to the individual sub-documents for the exact wording of the specification.</p>
    <h2 id="osd-contributions-and-licensing">OSD Contributions and Licensing</h2>
    <p>OSD is a community effort and anyone is invited to join. If you are interested in giving input, reviewing our specifications or joining the Open SoC Debug team, please visit our website: <a href="http://www.opensocdebug.org" class="uri">http://www.opensocdebug.org</a></p>
    <h3 id="specification-license">Specification License</h3>
    <p>This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License. To view a copy of this license, visit <a href="http://creativecommons.org/licenses/by-sa/4.0/" class="uri">http://creativecommons.org/licenses/by-sa/4.0/</a> or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.</p>
    <p>You are free to share and adapt this work for any purpose as long as you follow the following terms: (i) Attribution: You must give appropriate credit and indicate if changes were made, (ii) ShareAlike: If you modify or derive from this work, you must distribute it under the same license as the original.</p>
    <h2 id="revision-history">Revision History</h2>
    <h3 id="version-2016.1-preview-2-to-be-released">Version 2016.1, Preview 2 (to be released)</h3>
    <p>2nd preview of initial version for discussion.</p>
    <ul>
    <li>Revised document structure.</li>
    <li>Added more introduction and clarified design goals.</li>
    </ul>
    <h3 id="version-2016.1-preview-1-released-2016-02-01">Version 2016.1, Preview 1 (released 2016-02-01)</h3>
    <p>1st preview of initial version for discussion.</p>
    <p>A full revision history in all detail is available in our <a href="https://github.com/opensocdebug/documentation">Git repository</a>.</p>
    <h1 id="high-level-features">High-Level Features</h1>
    <p>By implementing OSD, a SoC can easily be enhanced with advanced debug functionality. This section describes these features in more detail.</p>
    <h2 id="run-control-debug">Run-Control Debug</h2>
    <p>Run-control debugging, a.k.a. breakpoint debugging, “stop-and-stare” debugging, or just “debugging,” is the most common way of finding problems in software at early stages of development. Using software tools like the GNU Debugger (gdb) breakpoints can be set in the software code. If this point in the program reached, the program execution is stalled and the program control is handed over to the debugger. Using the debugger, a developer can now read register or memory values, print stack traces, and much more. To be efficient, run-control debugging functionality needs hardware support to stop the program execution at a given time. In addition, run-control debugging on SoC platforms is usually done remotely, i.e. the system is controlled from a host PC, as opposed to running the debugger directly on the SoC.</p>
    <p>OSD contains all parts to add run-control debug support to a SoC. On the hardware side, OSD interfaces with the CPU core(s) to control its behavior. On the host side, OSD provides a daemon that GDB can connect to. The actual debugging is then handled by GDB and the usage of OSD is transparent to the software developer.</p>
    <h2 id="tracing">Tracing</h2>
    <p>Today’s heterogeneous multi-core designs present new challenges to software developers. Concurrent software distributed across multiple CPUs and hardware accelerators, interacting with complex I/O interfaces and strict real-time requirements is the new normal. This results in new classes of bugs which are hard to find, like race conditions, deadlocks, and severely degraded performance for no obvious reason. To find such bugs, run-control debugging is not applicable: setting a breakpoint disturbs the temporal relationship between the different threads of execution. This disturbance to the program execution is called “probe effect” and can cause the original problem to disappear when searching for it, a phenomenon known as “Heisenbug.”</p>
    <p>Tracing avoids these problems by unobtrusively monitoring the program execution and transferring the observations off-chip. There, the program flow can be reconstructed and the program behavior analyzed.</p>
    <p>OSD comes with components to enable tracing for not only CPU cores, but also for any component in the SoC, such as memories, hardware accelerators, and interconnects.</p>
    <h2 id="memory-access">Memory Access</h2>
    <p>Reading and writing memories is an essential tool during bring-up and debugging of a SoC. A typical use case is to write software to a program memory from the host PC, to avoid writing it for example to a SD card or flash memory and then resetting the system.</p>
    <p>OSD ships with a module that can be attached to a memory to support reads and writes from and to memories.</p>
    <h2 id="system-discovery">System Discovery</h2>
    <p>Users of today’s debug systems know the pain: setting up a debugger on a host PC to communicate with the hardware often requires obscure configuration settings, secret switches and a bit of magic sauce to make it all work.</p>
    <p>OSD is designed to be plug-and-play. All hardware components are self-describing. When a host connects to the system, it first enumerates all available components, and reads necessary configuration bits.</p>
    <h2 id="timestamping">Timestamping</h2>
    <p>Timestamps are monotonically increasing numbers which are attached to events generated by the debug system. (They usually do not correspond in any way to wall-clock time.) Timestamps enable correlation of events in different parts of the chip with each other. Additionally, they can be used to restore order to events which are (for some reason) out of order when they arrive.</p>
    <p>While timestamps are useful in many cases, adding them to all events generated by the debug system can significantly increase the overhead of such events.</p>
    <p>Currently OSD supports timestamps which are full numbers of configurable width. Some debug modules can be configured to enable or disable timestamp generation.</p>
    <p>The timestamping method used in OSD is referred to as “source timestamps” in some debug systems. Timestamps are added to the trace data at the source, as opposed to (e.g.) adding timestamps when the data is received by a debug adapter hardware between the SoC and the host PC.</p>
    <h2 id="security-and-authentication">Security and Authentication</h2>
    <p>Any debug system, by nature, exposes much of the system internals to the outside world. To prevent abuse of the debug system, production devices often require a developer to authenticate towards the system before being able to use the debug system.</p>
    <p>OSD provides the infrastructure to implement such features.</p>
    <h1 id="osd-by-example">OSD By Example</h1>
    <p>Before we dive into the details of the OSD architecture, this section discusses two typical usage examples of OSD. The first example only shows run-control debugging, the second one presents a full tracing infrastructure.</p>
    <h2 id="osd-for-run-control-debugging">OSD for Run-Control Debugging</h2>
    <p>Many smaller single-core designs traditionally only support run-control debugging through custom JTAG-based debug infrastructure. OSD supports this use case well. Its modular architecture makes it easy to implement only essential debug modules to support run-control debug, and to add advanced features such as trace later without major changes.</p>
    <figure>
    <img src="img/overview_example_debug.png" title="An example system using OSD for run-control debugging" alt="Figure 1: An example system using OSD for run-control debugging" id="fig:overview_example_debug" /><figcaption>Figure 1: An example system using OSD for run-control debugging</figcaption>
    </figure>
    <p>Fig. 1 shows an example configuration of OSD for a small run-control debug scenario. The functional system (to be attached on the right side) consists of a single-core CPU, a memory and a bus interconnect. To this functional system the debug modules are attached.</p>
    <ul>
    <li>The System Control Module (SCM) module allows to control the system remotely: reset the system, halt the system, reset the CPUs, etc.</li>
    <li>The Core Debug Module (CDM) provides all functionality expected from a run-control debug system: setting breakpoints and reading CPU registers.</li>
    <li>The Memory Access Module (MAM) gives access to the chip’s memories for loading the memories during debugging (e.g. with the program code), to verify the memory contents, or to read out memory contents during debugging.</li>
    <li>To show the benefits of using OSD, the example system adds another module, the Device Emulation Module UART (DEM-UART). This module behaves on the functional hardware side, and on the software side like a usual UART device. But instead of using dedicated pins, the data is transported through the debug connection.</li>
    </ul>
    <p>For all mentioned components, OSD includes a full specifications which enables a custom implementation, as well as a hardware implementation that can be used unmodified or adapted to fit the interface to the custom functional system.</p>
    <p>The debug modules are all connected to a debug network. The OSD specification does not require a specific network topology or implementation type. However, usually OSD implementations use a 16-bit wide, unidirectional ring network on chip (NoC), as it presents a good trade-off between area usage and performance.</p>
    <p>To connect with a host PC, three further components are needed: the Host Interface Module (HIM) on the hardware side, a GLIP transport module, and a software daemon on the host side.</p>
    <p>The transport of data between host and device is handled by <a href="http://glip.io">GLIP</a>. GLIP is a library which abstracts the data transport between hardware and software with a bi-directional FIFO interface. The data transport itself can happen through different physical interfaces, such as UART, JTAG, USB or PCI Express (PCIe). In the presented example, a JTAG connection is used. A possibly existing JTAG boundary scan interface can be re-used and a new Test Access Point (TAP) is added to the JTAG chain for the debug connection.</p>
    <p>The Host Interface Module (HIM) connects the debug network to the FIFO-interface of GLIP.</p>
    <p>On the software side, the OSD host daemon encapsulates the communication to the device and provides a API for various tools communicating with the debug system. A scriptable command line interface can be used to control the system (such as reset, halt, etc.) and to read and write memories. A gdb server provides an interface to the core debug functionality that the GNU Debugger (gdb) can connect to. In the end, software can be debugged with an unmodified gdb (and other gdb-enabled IDEs, such as Eclipse CDT).</p>
    <h2 id="osd-for-tracing">OSD for Tracing</h2>
    <p>Today’s debug system architectures strictly separate between run-control debugging and tracing. The example below shows how OSD units the two worlds with a common interface, thus reducing development and maintainance effort. Since most of the architecture is shared between run-control debugging and tracing, upgrading an existing design from run-control debugging to tracing is not a large step.</p>
    <figure>
    <img src="img/overview_example_trace.png" title="An example system dual-core system using OSD tracing" alt="Figure 2: An example system using OSD for run-control debugging" id="fig:overview_example_trace" /><figcaption>Figure 2: An example system using OSD for run-control debugging</figcaption>
    </figure>
    <p>Fig. 2 shows an example architecture of a OSD system with tracing support for a dual-core design. Most of the architecture is identical to the previous example: the host daemon, the HIM, the debug network and the SCM, CDM and MAM debug modules. New in this example are the following parts.</p>
    <ul>
    <li>The GLIP transport library now uses USB 2.0 instead of JTAG for communication. This allows for higher off-chip transfer speeds to get improved visibility into the system by tracing.</li>
    <li>The Core Trace Module (CTM) provides program trace (a.k.a. instruction trace) support. It is attached to the CPU core next to the CDM.</li>
    <li>A graphical trace viewer can be attached to the host daemon to view the traces. Currently, OSD does not come with such a tool, but all interfaces are provided to easily write such a tool.</li>
    </ul>
    <p>The two examples in this section have already shed a light on what is possible with OSD. In the remainder of this document, we’ll discuss OSD in more depth, starting with a more general overview of the architecture.</p>
    <h1 id="the-open-soc-debug-architecture">The Open SoC Debug Architecture</h1>
    <figure>
    <img src="img/overview.png" title="Debug System Overview" alt="Figure 3: Overview of an Open SoC Debug debug system" id="fig:overview" /><figcaption>Figure 3: Overview of an Open SoC Debug debug system</figcaption>
    </figure>
    <p>Fig. 3 shows the different components in an Open SoC Debug-based debug system.</p>
    <ul>
    <li><strong>Debug modules</strong> (shown on the right) monitor or interact with the functional components of the SoC. Towards the functional SoC the interface is implementation-specific. Towards the debug network the modules conform to a well-defined interface, consisting of two parts: a register-mapped control interface, and an event data interface (i.e. to send out trace data or other unsolicited messages).</li>
    <li>The <strong>debug network</strong> is used to exchange messages between the debug modules and the host.</li>
    <li>The <strong>physical transport</strong> connects the device to a host PC. For most implementations, we recommend using <a href="http://glip.io">GLIP</a>.</li>
    <li>On the host side, the <strong>OSD host library</strong> (<code>libopensocdebug</code>) provides a programming interface (API) to the debug system.</li>
    <li>On top of that library, the <strong>OSD daemon</strong> (<code>opensocdebugd</code>) can be used to enable multiple debug tools to interact with the on-chip debug system.</li>
    <li>Finally, <strong>debug tools</strong> use the debug system to perform debugging and analysis tasks, ranging from run-control debugging to tracing and runtime verification.</li>
    </ul>
    <p>All parts of the OSD architecture have been designed with extensibility in mind. But if no or only small customizations are needed, OSD also includes default implementations of most components which can be used to get a system up and running quickly.</p>
    <h2 id="debug-modules">Debug Modules</h2>
    <p>The debug modules either monitor a debug module or interact with it in case of run-control debugging or special functionalities. On the other side the debug modules generally interface the debug infrastructure via the so called “Debug Interconnect Interface” (DII).</p>
    <figure>
    <img src="img/debug_module_generic_if.png" title="Generic status and control interface" alt="Figure 4: The generic status and control interface of all debug modules" id="fig:debug_module_generic_if" /><figcaption>Figure 4: The generic status and control interface of all debug modules</figcaption>
    </figure>
    <p>All debug modules have a common debug-side status and control interface as depicted in fig. 4. It is a base register map that contains mandatory and optional registers such as:</p>
    <ul>
    <li>The <em>module class</em> and a module <em>vendor id</em> and <em>product id</em>, that support enumeration and software handling of the debug modules on the host</li>
    <li>Enable and disable the entire module</li>
    <li>Query module-specific capabilities and enable features</li>
    </ul>
    <p>This interface usually runs in the debug system clock domain, while the actual module logic is responsible for clock domain crossing between the connected system component and the debug clock domain. Most simply, tracing is usually done by (naturally) sampling the trace information in the component’s domain and cross the trace event via a small buffer into the module logic that handles packetization of the trace event.</p>
    <h3 id="register-access">Register Access</h3>
    <p>The host sends register access packets to the debug modules to</p>
    <ul>
    <li>read and write control &amp; status registers, or</li>
    <li>access a debug module functionality</li>
    </ul>
    <p>For example, the host can send a <code>REQ_READ_REG</code> packet to read module version from the defined memory address <code>MOD_VERSION (0x1)</code>. The module will reply with a <code>RESP_READ_REG</code> containing the module version.</p>
    <p>It is generally allowed that debug modules can also generate such request to query or control other debug modules.</p>
    <h4 id="memory-mapped-io-mmio-bridge">Memory-Mapped I/O (MMIO) Bridge</h4>
    <figure>
    <img src="img/debug_module_mmiobridge.png" title="The convenience MMIO bridge wrapper" alt="Figure 5: The MMIO convenience wrapper." id="fig:debug_module_mmiobridge" /><figcaption>Figure 5: The MMIO convenience wrapper.</figcaption>
    </figure>
    <p>OSD comes with convenience wrapper that maps the register access debug packets to memory-mapped bus interface. As depicted in fig. 5 this module is especially useful for host-controlled modules, such as run-control debugging.</p>
    <p>The basic bus interface allows for register addresses. The data width is configurable, for example as a processor’s data width. The memory addresses are register numbers, so that is is not possible to address unaligned to the configured data width.</p>
    <p>Finally, there is an <code>interrupt</code> signal that can be used to send unsolicited events to the host, for example a <code>breakpoint</code> event. The bridge is configured to read a value from a configured address and send it to the host. Thereby it is possible to implement run-control debugging without polling for events.</p>
    <h3 id="debug-events">Debug Events</h3>
    <p>Debug events are unsolicited messages generated from a debug module. This can for examle be a “breakpoint hit” message from a run-control debug module, or a trace message. In the first case the host usually starts with a sequence of register accesses, while in general debug events are of a fire-and-forget nature.</p>
    <h3 id="trace-modules">Trace Modules</h3>
    <figure>
    <img src="img/generic_trace_module.png" title="Generic trace module structure" alt="Figure 6: Generic trace module structure" id="fig:generic_trace_module" /><figcaption>Figure 6: Generic trace module structure</figcaption>
    </figure>
    <p>Trace modules have an overall structure as depicted in fig. 6. Their task is to sample trace events generated by the hardware. This trace events can be of arbitrary sizes, but are usually constant at a single trace module’s sampling interface. Examples are:</p>
    <ul>
    <li>A processor core’s execution trace: Executed program counter, branch-taken or similar</li>
    <li>A processor core’s diagnosis trace: Functional unit utilization, branch predictor efficiency, etc.</li>
    <li>Cache diagnosis trace: Hits/Misses, Conflicts, average memory access time, etc.</li>
    <li>DMA controller trace: Start and end of request, average memory latency</li>
    </ul>
    <p>Summarizing, nearly every hardware block is a candidate to generate useful trace information.</p>
    <h3 id="clock-domains">Clock Domains</h3>
    <p>The base functionality of a trace module is packetization of the trace data to trace event packets. Optionally, the module may filter events or compress the event stream. At some point it is necessary to cross between the module’s clock domain and the debug clock domain. This can alternatively be done on the trace event sampling, at the packet output or somewhere in between, depending on which clock is faster and at which rates trace events are generated.</p>
    <h3 id="overflow-handling">Overflow Handling</h3>
    <p>In case the trace events are generated at a faster rate than the host interface can transfer. This problem becomes crucial with the increasing number of trace modules. Generally, this can be done on the level of the debug system by a sophisticated flow control that will be specified in later revisions. An overflow occurs if a trace event is generated, but cannot be transferred or buffered due to backpressure from the interconnect, but backpressure cannot be generated to the system module. In the current specification the trace infrastructure detects this situation, counts how many packets could not be transfered and then transfers a <code>missed_events</code> event once it the interface is available again.</p>
    <h2 id="transport-and-switching">Transport and Switching</h2>
    <p>To route debug information to the correct debug module and to the host, OSD uses a simple packet-based protocol. The packet size is limited by the implementation and can be queried from the <em>System Control Module (SCM)</em>. The minimum value for the maximum packet size is 8.</p>
    <p>The <em>Debug Interconnect Interface (DII)</em> defines the data format and flow control mechanism. The packet width must be at least 16 bit and currently is set fixed to 16 bit.</p>
    <p>The debug packets contain the necessary routing and identification information, namely the destination and the source, in their header, which are the first two data items in a packet.</p>
    <p>One key property of the transport &amp; switching in the Open SoC Debug specification is that it generally allows that debug modules exchange packets between them. This enables on-chip trace processing, run-control debugging from a special core or other methods to reduce the traffic on the host interface, which is the most critical resource in modern debugging.</p>
    <figure>
    <img src="img/interconnect.png" title="Debug ring and other interconnects" alt="Figure 7: Debug ring and other interconnects" id="fig:interconnect" /><figcaption>Figure 7: Debug ring and other interconnects</figcaption>
    </figure>
    <p>In general, the interconnect can have any possible topology as long as it fulfills two basic properties: strict-ordering of packets with the same <code>(src,dest)</code> tuple and deadlock-freedom. The first property does forbid debug packets from one source to one destination to overtake each other in the interconnect to allow payload data to span multiple packets. Fig. 7 shows the favored topologies. The baseline implementation is a simple ring interconnect. The ring balances well between clock speed, required chip area and most importantly flexibility. It can easily span the entire chip without dominating a design.</p>
    <p>Alternatively, other topologies may be favored. For example a low count of debug modules favors a multiplexer interconnect. Especially if the debug modules are all trace debugging or all run-control debugging a bus or similar can be favorable for low debug module counts. When the modules also communicate with each other a crossbar may be used for high throughput, but with the disadvantage of large area overhead.</p>
    <p>Finally, we believe once some first tests with larger systems in the real world have been performed, hierarchical topologies may become favorable. Beside optimizing the resource utilization, aggregating modules may bridge subsets of trace modules to the actual debug interconnect to perform size optimizations on the aggregated packet stream.</p>
    <h2 id="physical-interface">Physical Interface</h2>
    <p>The physical interface is abstracted in Open SoC Debug as a FIFOs which transmit data between the host and the device.</p>
    <figure>
    <img src="img/glip.png" title="GLIP abstracts from the physical interface" alt="Figure 8: GLIP as abstraction layer from the physical interface" id="fig:glip_overview" /><figcaption>Figure 8: GLIP as abstraction layer from the physical interface</figcaption>
    </figure>
    <p>While not required by OSD, we recommend building on top of <a href="http://www.glip.io">GLIP</a> as depicted in fig. 8. GLIP provides a generic FIFO interface that reliably transfers data between the host and the system. Multiple alternatives for simulations and prototyping hardware exist. In a silicon device, a high-speed serial interface is most probably favorable.</p>
    <h2 id="host-software">Host Software</h2>
    <p>As mentioned before, the host software is not in the focus of the Open SoC Debug project, but we strongly support development of debug software around our infrastructure.</p>
    <p>The basic level of the <code>libopensocdebug</code> is the packetization of debug packets. It also provides higher-level functions, for example register access functions or up to convenience functions to perform module-specific operations. A debug tool can build against this library, or alternatively start the <code>opensocdebugd</code> daemon that allows multiplexing of one Open SoC Debug-enabled system between different tools.</p>
    <h1 id="basic-debug-modules">Basic Debug Modules</h1>
    <p>In the following we will shortly introduce the core group of debug modules which are be part of Open SoC Debug. Only two modules are mandatory: The <em>Host Interface Module</em> to transfer data between the debug system and the host or memory, and the <em>System Control Module</em> that identifies the system, provides system details and controls the system.</p>
    <h2 id="host-interface-module-him">Host Interface Module (HIM)</h2>
    <figure>
    <img src="img/debug_module_him.png" title="Host Interface Module" alt="Figure 9: Host Interface Module" id="fig:debug_module_him" /><figcaption>Figure 9: Host Interface Module</figcaption>
    </figure>
    <p>The <em>Host Interface Module (HIM)</em> converts the debug packets to a <code>length</code>-<code>value</code> encoded data stream, that is transferred using the glip interconnect. This format is simple and contains the length of the debug packet in one data item followed by the debug packet.</p>
    <p>Alternatively, the HIM can be configured to store the debug packets to the system memory using the memory interface.</p>
    <h2 id="host-authentication-module-ham">Host Authentication Module (HAM)</h2>
    <figure>
    <img src="img/debug_module_ham.png" title="Host Authentication Module" alt="Figure 10: Host Authentication Module" id="fig:debug_module_ham" /><figcaption>Figure 10: Host Authentication Module</figcaption>
    </figure>
    <p>The system can require the host to authenticate before connecting to the debug system, because the debug can expose confidential information. A <em>HAM</em> implementation can for example require a token to match or a sophisticated challenge-response protocol. If configured the <a href="#host-interface-module-him">HIM</a> will wait for the HAM to allow the host to communicate with modules other than the HAM.</p>
    <h2 id="system-control-module-scm">System Control Module (SCM)</h2>
    <figure>
    <img src="img/debug_module_scm.png" title="System Control Module" alt="Figure 11: System Control Module" id="fig:debug_module_scm" /><figcaption>Figure 11: System Control Module</figcaption>
    </figure>
    <p>The <em>System Control Module (SCM)</em> is always mapped to address <code>1</code> on the debug interconnect (<code>0</code> is the host/HIM address). The host first queries the SCM to provide system information, like a system identifier, the number of debug modules, or the maximum packet length.</p>
    <p>Beside that it can be used to control the system. For that it can set the soft reset of the processor cores and the peripherals separately in the first specification.</p>
    <h2 id="core-debug-module-cdm">Core Debug Module (CDM)</h2>
    <figure>
    <img src="img/debug_module_cdm.png" title="Core Debug Module" alt="Figure 12: Core Debug Module" id="fig:debug_module_cdm" /><figcaption>Figure 12: Core Debug Module</figcaption>
    </figure>
    <p>The core debug module implements run-control debugging for a processor core. The implementation is to a certain degree core-dependent, but a generic implementation is sketched in fig. 12. It has a memory mapped interface as described above. The debug control, status information and core register are mapped in memory regions. The run-control debugger (e.g., gdb) then sends register access requests. In case of a debug event (breakpoint hit) <code>interrupt</code> signals are asserted. As a reaction the CDM reads a defined address and the core-specific part of the CDM generates a debug event.</p>
    <p>Of course, other implementations are possible or may be required depending on the interface processor implementation.</p>
    <h2 id="core-trace-module-ctm">Core Trace Module (CTM)</h2>
    <p>The <em>Core Trace Module (CTM)</em> captures trace events generated from the processor core. The implementation is core-dependent and will be highly configurable. Such trace events are core-internal signals, like the completion of an instruction, the branch predictor status, memory access delays, cache miss rates, just to name a few possibilities.</p>
    <p>The CTM specification will define a few basic trace events and how they can efficiently packed, because such events are usually generated with a high rate.</p>
    <h2 id="software-trace-module-stm">Software Trace Module (STM)</h2>
    <p>The <em>Software Trace Module (STM)</em> emits trace events that are emitted by the software execution. Such an STM event is a tuple <code>(id,value)</code>. There are generally two classed: user-defined and system-generated trace events.</p>
    <p>User-defined trace events are added by the user by instrumenting the source code with calls to an API like <code>TRACE(short id, uint64_t value)</code>. A debug tool can map the trace events to a visualization.</p>
    <p>Different user threads can emit trace events interleaved. Beside this the operating system can emit relevant trace information too. For both reasons, there are system-generated events.</p>
    <p>There are two ways to emit a software trace event. First there is a set of <em>special purpose registers</em> or similar techniques used to emit trace events. Most importantly, each trace event must be emitted atomically. Secondly, the processor core can have hardware to emit software trace events. For example a mode change can be emitted without much overhead.</p>
    <p>The generic trace interface is <code>enable</code>, <code>id</code> and <code>value</code> at the core level and the STM handles the filtering, aggregation and packetization as described above.</p>
    <h2 id="memory-access-module-mam">Memory Access Module (MAM)</h2>
    <figure>
    <img src="img/debug_module_mam.png" title="Memory Access Module" alt="Figure 13: Memory Access Module" id="fig:debug_module_mam" /><figcaption>Figure 13: Memory Access Module</figcaption>
    </figure>
    <p>The <em>Memory Access module (MAM)</em> is used to write data to the memory or read data back from the memory. This module can therefore be used to initialize the memory with a program or inspect the memory post-mortem or during run-control debugging.</p>
    <p>The module is either connected to the system memory, other memory blocks or the last level cache. In the presence of write-back caches the memory access may be required to be guarded by a run-control triggered forced writeback if necessary.</p>
    <h2 id="debug-processor-module-dpm">Debug Processor Module (DPM)</h2>
    <figure>
    <img src="img/debug_module_dpm.png" title="Debug Processor Module" alt="Figure 14: Debug Processor Module" id="fig:debug_module_dpm" /><figcaption>Figure 14: Debug Processor Module</figcaption>
    </figure>
    <p>As mentioned in the <a href="#introduction">Introduction</a> we believe in the importance of on-chip processing of debug information. The chip interface is the bottleneck in the entire debug infrastructure. But the developer wants to collect as much trace events as possible to get a complete picture of the execution. The approach to solve this trade-off is to process trace events on the chip already. This can be either filtering or compression as introduced with <a href="#trace-modules">Trace Modules</a>, but also more complex processing of trace events to generate processed information out of raw data.</p>
    <p>A debug processor module is a subsystem in the debug system that can receive debug packets, store them and process them to new debug packets to be sent to the host. As depicted in fig. 14 a basic DPM therefore contain a programmable hardware block (possibly a simple CPU) and some local memory to execute programs from and store debug data. A DPM can also send debug packets to configure debug modules and set itself as destination of packets or configure filters, etc.</p>
    <p>This subsystem may be interface from the system itself to configure it. It may also be part of the actual system, like a core that can be dynamically dedicated to be a DPM.</p>
    <h2 id="device-emulation-module-dem">Device Emulation Module (DEM)</h2>
    <p>It may be desired to deploy I/O modules that do not map to I/O pins, but instead exchange transactions with the host. This may for example be a serial terminal that send output characters to the host. Another important use case for such modules is the emulation of devices on the host or the simulation of a module during development of it.</p>
  </div>
</div>
